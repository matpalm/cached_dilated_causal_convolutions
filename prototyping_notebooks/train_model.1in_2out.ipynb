{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3605364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import cmsisdsp as dsp\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mat/dev/cached_dilated_causal_convolutions/') \n",
    "\n",
    "from cmsisdsp_py_version.block import Block\n",
    "from cmsisdsp_py_version.keras_model import create_dilated_model\n",
    "from cmsisdsp_py_version.cached_block_model import CachedBlockModel, Classifier\n",
    "from cmsisdsp_py_version.rolling_cache import RollingCache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    df_w = pd.read_csv(fname, sep=' ', names=['in', 'out_l', 'out_r'])\n",
    "    df_w['n'] = range(len(df_w))\n",
    "    df_l = df_w.melt(id_vars='n', value_vars=['in', 'out_l', 'out_r'])\n",
    "    return df_w, df_l\n",
    "\n",
    "sine_df_w, sine_df_l = parse('../datalogger_firmware/sine.ssv')\n",
    "saw_df_w, saw_df_l = parse('../datalogger_firmware/saw.ssv')\n",
    "square_df_w, square_df_l = parse('../datalogger_firmware/square.ssv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "offset = 5500\n",
    "width = 3000\n",
    "df = sine_df_l\n",
    "window = (df['n']>offset) & (df['n']<offset+width)\n",
    "sns.lineplot(df[window], x='n', y='value', hue='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "offset = 100\n",
    "width = 2000\n",
    "df = saw_df_l\n",
    "window = (df['n']>offset) & (df['n']<offset+width)\n",
    "sns.lineplot(df[window], x='n', y='value', hue='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c858c9",
   "metadata": {},
   "source": [
    "some super weird stuff going on with the square wave :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "offset = 4000\n",
    "width = 1000\n",
    "df = square_df_l\n",
    "window = (df['n']>offset) & (df['n']<offset+width)\n",
    "sns.lineplot(df[window], x='n', y='value', hue='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1b1d0",
   "metadata": {},
   "source": [
    "training with sine worked, but the wave was boring\n",
    "let's try the square next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(df_w):\n",
    "    data = df_w.to_numpy()\n",
    "    assert data.shape[1] == 4  # 4 columns\n",
    "    x = data[:,0:1]\n",
    "    y_true = data[:,1:3]\n",
    "\n",
    "    split = int(len(x) * 0.05)\n",
    "    train_x, train_y = x[:(2*-split)], y_true[:(2*-split)]\n",
    "    validate_x, validate_y = x[(2*-split):-split], y_true[(2*-split):-split]\n",
    "    test_x, test_y = x[-split:], y_true[-split:]\n",
    "\n",
    "    return {'train_x': train_x, 'train_y': train_y, \n",
    "            'validate_x': validate_x, 'validate_y': validate_y,\n",
    "            'test_x': test_x, 'test_y': test_y }\n",
    "\n",
    "sine_splits = splits(sine_df_w)\n",
    "saw_splits = splits(saw_df_w)\n",
    "square_splits = splits(square_df_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mono in -> stereo out\n",
    "IN_D = 1\n",
    "OUT_D = 2\n",
    "\n",
    "# kernel size and implied dilation rate\n",
    "K = 4                \n",
    "\n",
    "# filters for Nth layer Kx1 and 1x1 convs\n",
    "# [4, 8, 8, 8] @ 32kHz => 82%\n",
    "# [4, 8, 8, 12] @ 32kHz => 93%\n",
    "FILTER_SIZES = [4, 8, 8, 12]  \n",
    "\n",
    "RECEPTIVE_FIELD_SIZE = K**len(FILTER_SIZES)\n",
    "\n",
    "TEST_SEQ_LEN = RECEPTIVE_FIELD_SIZE\n",
    "TRAIN_SEQ_LEN = RECEPTIVE_FIELD_SIZE * 10\n",
    "\n",
    "print(\"RECEPTIVE_FIELD_SIZE\", RECEPTIVE_FIELD_SIZE)\n",
    "print(\"TRAIN_SEQ_LEN\", TRAIN_SEQ_LEN)\n",
    "print(\"TEST_SEQ_LEN\", TEST_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configured_keras_model(seq_len, all_outputs):\n",
    "    return create_dilated_model(\n",
    "        seq_len, in_d=IN_D, filter_sizes=FILTER_SIZES,\n",
    "        kernel_size=K, out_d=OUT_D,\n",
    "        all_outputs=all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = create_configured_keras_model(TRAIN_SEQ_LEN, all_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6d514-dd4a-4dbe-b716-3479165ea496",
   "metadata": {},
   "source": [
    "we want to ignore the first RECEPTIVE_FIELD_SIZE elements of the loss since they have been fed with left padded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d5a92-de4c-40c5-886f-4cc817d03f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.concatenate([np.zeros(RECEPTIVE_FIELD_SIZE), \n",
    "                       np.ones(TRAIN_SEQ_LEN-RECEPTIVE_FIELD_SIZE)])\n",
    "mask = tf.convert_to_tensor(mask.astype(np.float32))\n",
    "\n",
    "def masked_mse(y_true, y_pred):    \n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "    return tf.multiply(mse, mask)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715618af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# which splits to use for train/validate\n",
    "splits = sine_splits\n",
    "\n",
    "def dataset_from(x, y):\n",
    "    def gen():\n",
    "        # TODO: could shuffle x and y here by indexes\n",
    "        for i in range(len(x)-TRAIN_SEQ_LEN-1):\n",
    "            yield x[i:i+TRAIN_SEQ_LEN], y[i+1:i+1+TRAIN_SEQ_LEN]\n",
    "                \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen, output_signature=(tf.TensorSpec(shape=(TRAIN_SEQ_LEN, IN_D), dtype=tf.float32),\n",
    "                               tf.TensorSpec(shape=(TRAIN_SEQ_LEN, OUT_D), dtype=tf.float32)))\n",
    "    return ds\n",
    "\n",
    "train_ds = tf.data.Dataset.sample_from_datasets([\n",
    "    dataset_from(sine_splits['train_x'], sine_splits['train_y']).cache().shuffle(10000),\n",
    "    dataset_from(saw_splits['train_x'], saw_splits['train_y']).cache().shuffle(10000),\n",
    "    dataset_from(square_splits['train_x'], square_splits['train_y']).cache().shuffle(10000),\n",
    "])\n",
    "train_ds = train_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# train_ds = dataset_from(splits['train_x'], splits['train_y'])\n",
    "# train_ds = train_ds.cache().shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def scheduler(epoch, lr):    \n",
    "    return 1e-4 if epoch == 1 else 1e-5    \n",
    "sch_callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "validate_ds = dataset_from(splits['validate_x'], splits['validate_y'])\n",
    "validate_ds = validate_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_model.compile(Adam(1e-4), loss=masked_mse)\n",
    "train_model.fit(train_ds, \n",
    "                validation_data=validate_ds,\n",
    "                callbacks=[sch_callback],\n",
    "                epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39227ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.save(\"trained_models/all_three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6d1fc-0c81-4c6c-a634-8a338beea902",
   "metadata": {},
   "source": [
    "# check against test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3519e-d3e3-4278-a13b-9d8ea56b82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_records = []\n",
    "\n",
    "# which split to use for test\n",
    "splits = sine_splits\n",
    "\n",
    "for i in range(2000, 3000):\n",
    "    test_seq = splits['test_x'][i:i+TRAIN_SEQ_LEN]\n",
    "    test_seq = np.expand_dims(test_seq, 0)  # single element batch\n",
    "    \n",
    "    y_true = splits['test_y'][i+TRAIN_SEQ_LEN]\n",
    "            \n",
    "    y_pred = train_model(test_seq).numpy().squeeze()\n",
    "    y_pred = y_pred[-1]  # train model gives all steps, we just want last\n",
    "    \n",
    "    for c in range(2):\n",
    "        test_records.append((i, c, 'test_x', test_seq[0,-1,0])) \n",
    "        test_records.append((i, c, 'y_true', y_true[c]))\n",
    "        test_records.append((i, c, 'y_pred', y_pred[c]))\n",
    "    \n",
    "test_df = pd.DataFrame(test_records, columns=['n', 'c', 'name', 'value'])\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.lineplot(data=test_df[test_df['c']==1], x='n', y='value', hue='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe473b9-fcc3-44ad-b4e8-12027f375bbe",
   "metadata": {},
   "source": [
    "## generate code\n",
    "\n",
    "hacktastically generate some blocks of c++ code for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ad19b-3de6-43e9-bfc6-855dbb113354",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_model.layers) == 10, len(train_model.layers)\n",
    "\n",
    "# layer[0] is input\n",
    "\n",
    "blocks = [\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[1].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[1].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[2].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[2].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[3].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[3].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[4].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[4].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[5].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[5].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[6].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[6].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[7].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[7].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[8].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[8].weights[1].numpy(),\n",
    "    )\n",
    "]\n",
    "\n",
    "classifier = Classifier(\n",
    "    weights=train_model.layers[9].weights[0].numpy()[0],\n",
    "    biases=train_model.layers[9].weights[1].numpy()   \n",
    ")\n",
    "\n",
    "# create CachedBlockModel since it creates correct layer\n",
    "# caches\n",
    "cached_block_model = CachedBlockModel(\n",
    "    blocks=blocks,\n",
    "    input_feature_depth=IN_D,\n",
    "    classifier=classifier\n",
    ")      \n",
    "\n",
    "with open(\"/tmp/model_defn.h\", 'w') as f:\n",
    "    cached_block_model.write_model_defn_h(f) #sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc641867-bcc1-4861-8afb-5622ec67595e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
