{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459f896d-d7b0-4bb6-ae4f-90d14bfab2ce",
   "metadata": {},
   "source": [
    "# see keras_model.train instead\n",
    "\n",
    "note: still to port is the single block average gradient code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3605364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import cmsisdsp as dsp\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mat/dev/cached_dilated_causal_convolutions/') \n",
    "\n",
    "from cmsisdsp_py_version.block import Block\n",
    "from cmsisdsp_py_version.keras_model import create_dilated_model, create_strided_model\n",
    "from cmsisdsp_py_version.cached_block_model import CachedBlockModel, Regression\n",
    "from cmsisdsp_py_version.rolling_cache import RollingCache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(fname):\n",
    "    df_w = pd.read_csv(fname, sep=' ', names=['tri', 'w0', 'w1'])\n",
    "    df_w['n'] = range(len(df_w))\n",
    "    df_l = df_w.melt(id_vars='n', value_vars=['tri', 'w0', 'w1'])\n",
    "    return df_w, df_l\n",
    "\n",
    "tsr_df_w, tsr_df_l = parse('../datalogger_firmware/data/2d_embed/32kHz/tri_sine_ramp.ssv')\n",
    "tsz_df_w, tsz_df_l = parse('../datalogger_firmware/data/2d_embed/32kHz/tri_squ_zigzag.ssv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "offset = 5500\n",
    "width = 1000\n",
    "df = tsr_df_l\n",
    "window = (df['n']>offset) & (df['n']<offset+width)\n",
    "sns.lineplot(df[window], x='n', y='value', hue='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac397e6-9a09-4e06-b050-ec7a4722c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "offset = 5500\n",
    "width = 1000\n",
    "df = tsz_df_l\n",
    "window = (df['n']>offset) & (df['n']<offset+width)\n",
    "sns.lineplot(df[window], x='n', y='value', hue='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00199ded-aca9-404f-9831-a3fea94ac89c",
   "metadata": {},
   "source": [
    "we want to rebuild the dataset from tri, sine, ramp and tri, square, zigzag\n",
    "to (embed0, embed1, tri) -> one of the other waves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b38e0-34af-4059-a22b-39920c56aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tsr_df_w.to_numpy().astype(np.float32)\n",
    "\n",
    "tri_to = {}\n",
    "tri_to['sine'] = {}\n",
    "tri_to['sine']['x'] = np.empty((len(data), 3), dtype=np.float32)\n",
    "tri_to['sine']['x'][:,0] = 0  # x2\n",
    "tri_to['sine']['x'][:,1] = 0  # x3\n",
    "tri_to['sine']['x'][:,2] = data[:,0] # triangle\n",
    "tri_to['sine']['y'] = np.expand_dims(data[:,1], -1) # sine\n",
    "\n",
    "tri_to['ramp'] = {}\n",
    "tri_to['ramp']['x'] = np.empty((len(data), 3), dtype=np.float32)\n",
    "tri_to['ramp']['x'][:,0] = 0  # x2\n",
    "tri_to['ramp']['x'][:,1] = 1  # x3\n",
    "tri_to['ramp']['x'][:,2] = data[:,0] # triangle\n",
    "tri_to['ramp']['y'] = np.expand_dims(data[:,2], -1) # ramp\n",
    "\n",
    "data = tsz_df_w.to_numpy().astype(np.float32)\n",
    "\n",
    "tri_to['square'] = {}\n",
    "tri_to['square']['x'] = np.empty((len(data), 3), dtype=np.float32)\n",
    "tri_to['square']['x'][:,0] = 1  # x2\n",
    "tri_to['square']['x'][:,1] = 0  # x3\n",
    "tri_to['square']['x'][:,2] = data[:,0] # triangle\n",
    "tri_to['square']['y'] = np.expand_dims(data[:,1], -1) # square\n",
    "\n",
    "tri_to['zigzag'] = {}\n",
    "tri_to['zigzag']['x'] = np.empty((len(data), 3), dtype=np.float32)\n",
    "tri_to['zigzag']['x'][:,0] = 1  # x2\n",
    "tri_to['zigzag']['x'][:,1] = 1  # x3\n",
    "tri_to['zigzag']['x'][:,2] = data[:,0] # triangle\n",
    "tri_to['zigzag']['y'] = np.expand_dims(data[:,2], -1) # zigzag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5686d0b-94eb-4890-9cc7-fbf5e4066c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(d):\n",
    "    assert 'x' in d\n",
    "    assert 'y' in d\n",
    "    assert len(d['x']) == len(d['y'])\n",
    "    val_test_split_size = int(len(d['x']) * 0.1)  # 10% for val and test\n",
    "    d['train'] = {}\n",
    "    d['validate'] = {}\n",
    "    d['test'] = {}\n",
    "    for xy in ['x', 'y']:                \n",
    "        d['train'][xy] = d[xy][:-2*val_test_split_size]        \n",
    "        d['validate'][xy] = d[xy][-2*val_test_split_size:-val_test_split_size]        \n",
    "        d['test'][xy] = d[xy][-val_test_split_size:]\n",
    "        d.pop(xy)\n",
    "\n",
    "for wave in ['sine', 'ramp', 'square', 'zigzag']:\n",
    "    split_train_val_test(tri_to[wave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce28d5-d669-4d27-83a1-1e8fdc8fe907",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:,0][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71798f85-c5e4-4f8c-8aea-90e51bd3596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tri_to['ramp']['train']['x'][:,2][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097991e8-c36c-418d-add4-47b9e7608036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tri_to['ramp']['validate']['y'][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1f49e-cd64-4097-a91b-39a964ee7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tri_to['square']['test']['y'][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tri_to['sine']['train']['y'][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc188ce-aaf9-46ef-af36-e3f6dd563d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tri_to['zigzag']['validate']['y'][1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_D = 3    # 2d embedding, (0,1) and core triangle\n",
    "OUT_D = 1   # output wave\n",
    "\n",
    "# kernel size and implied dilation rate\n",
    "K = 4                \n",
    "\n",
    "# filters for Nth layer Kx1 and 1x1 convs\n",
    "# [4, 3, 8, 8] @ 32kHz => 72%\n",
    "# [4, 8, 8, 8] @ 32kHz => 82%\n",
    "# [4, 8, 8, 12] @ 32kHz => 93%\n",
    "# [8, 8, 8, 8] @ 32kHz => TOO MUCH\n",
    "# [4, 4, 4] @ 96kHz => too much :/\n",
    "# [2, 2, 4] @ 96kHz => too much :/\n",
    "# [2, 2, 2] @ 96kHz => too much :/\n",
    "\n",
    "FILTER_SIZES = [4, 4, 4, 4]\n",
    "\n",
    "RECEPTIVE_FIELD_SIZE = K**len(FILTER_SIZES)\n",
    "\n",
    "TEST_SEQ_LEN = RECEPTIVE_FIELD_SIZE\n",
    "TRAIN_SEQ_LEN = RECEPTIVE_FIELD_SIZE * 5\n",
    "\n",
    "print(\"RECEPTIVE_FIELD_SIZE\", RECEPTIVE_FIELD_SIZE)\n",
    "print(\"TRAIN_SEQ_LEN\", TRAIN_SEQ_LEN)\n",
    "print(\"TEST_SEQ_LEN\", TEST_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configured_keras_model(seq_len, all_outputs):\n",
    "    return create_dilated_model(\n",
    "        seq_len, in_d=IN_D, filter_sizes=FILTER_SIZES,\n",
    "        kernel_size=K, out_d=OUT_D,\n",
    "        all_outputs=all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = create_configured_keras_model(TRAIN_SEQ_LEN, all_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035154ad-f3cf-4108-96f9-eed4dd2de591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strided_variant = create_strided_model(\n",
    "#         RECEPTIVE_FIELD_SIZE, in_d=IN_D, filter_sizes=FILTER_SIZES,\n",
    "#         kernel_size=K, out_d=OUT_D)\n",
    "# strided_variant.set_weights(train_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da061f33-b37f-4f22-9810-78fb7369a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a8fa1-a9fb-47a7-a742-f77be556b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def masked_mse(y_true, y_pred):    \n",
    "    assert len(y_true.shape) == 3, \"expected (batch, sequence_length, output_dim)\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    # average over elements of y\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)  \n",
    "    \n",
    "    # we want to ignore the first RECEPTIVE_FIELD_SIZE elements of the loss since they \n",
    "    # have been fed with left padded data\n",
    "    mse = mse[:,RECEPTIVE_FIELD_SIZE:]     \n",
    "\n",
    "    # return average over batch and sequence\n",
    "    return tf.reduce_mean(mse)\n",
    "    \n",
    "def dataset_from(x, y, s):\n",
    "    def gen():\n",
    "        idxs = list(range(len(x)-TRAIN_SEQ_LEN-1))  # ~1.3M\n",
    "        random.Random(1337).shuffle(idxs)\n",
    "        if s == 'train':            \n",
    "            idxs = idxs[:20_000]   # 200_000\n",
    "        else:\n",
    "            idxs = idxs[:500]   # 5_000\n",
    "        for i in idxs:\n",
    "            yield x[i:i+TRAIN_SEQ_LEN], y[i+1:i+1+TRAIN_SEQ_LEN]\n",
    "                \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen, output_signature=(tf.TensorSpec(shape=(TRAIN_SEQ_LEN, IN_D), dtype=tf.float32),\n",
    "                               tf.TensorSpec(shape=(TRAIN_SEQ_LEN, OUT_D), dtype=tf.float32)))\n",
    "    return ds\n",
    "\n",
    "def datasets_for_split(s):\n",
    "    return  [\n",
    "        dataset_from(tri_to[wave][s]['x'], tri_to[wave][s]['y'], s) #.cache() #filename=f\"tf_data_cache_{wave}\")\n",
    "        for wave in ['sine', 'ramp', 'square', 'zigzag']  \n",
    "    ] \n",
    "    \n",
    "train_ds = tf.data.Dataset.sample_from_datasets(datasets_for_split('train'))\n",
    "train_ds = train_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validate_ds = tf.data.Dataset.sample_from_datasets(datasets_for_split('validate'))\n",
    "validate_ds = validate_ds.batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715618af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard training loop\n",
    "\n",
    "def scheduler(epoch, lr): \n",
    "    return 1e-4\n",
    "    # if epoch <= 2:\n",
    "    #     return 1e-3\n",
    "    # elif epoch <= 4:\n",
    "    #     return 1e-4\n",
    "    # else:\n",
    "        # return 1e-5\n",
    "sch_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='weights/{epoch:03d}-{val_loss:.5f}',\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "train_model.compile(Adam(1e-4), loss=masked_mse)\n",
    "train_model.fit(train_ds, \n",
    "                validation_data=validate_ds,\n",
    "                callbacks=[sch_cb, checkpoint_cb],\n",
    "                epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f535adc-a55b-438b-a959-d5d39f4cd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same loop but with explicit gradient tape\n",
    "\n",
    "optimiser = Adam(1e-4) #, loss=masked_mse)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"epoch\", epoch)\n",
    "    \n",
    "    for x_b, y_b in train_ds:        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = train_model(x_b, training=True)\n",
    "            loss = masked_mse(y_b, y_pred)            \n",
    "        grads = tape.gradient(loss, train_model.trainable_weights)\n",
    "        optimiser.apply_gradients(zip(grads, train_model.trainable_weights))\n",
    "            \n",
    "    print(\"end epoch, final loss\", loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0af09-ee36-4e60-8a29-ee572bc878fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.compile(Adam(1e-4), loss=masked_mse)\n",
    "train_model.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dd018-4d9b-4fb4-83f0-a9f9545e97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.name for l in train_model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31563037-3871-4cb5-a7bc-a75f6b397e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_for(postfix, model):\n",
    "    idxs = []\n",
    "    for i, w in enumerate(model.trainable_weights):        \n",
    "        if w.name.startswith('c0'):\n",
    "            # always ignore c0 which is mapping input\n",
    "            continue\n",
    "        if postfix in w.name:\n",
    "            idxs.append(i)\n",
    "    assert len(idxs) == 3, len(idxs)\n",
    "    return idxs\n",
    "\n",
    "def average_gradients_for(postfix, model, grads):\n",
    "    idxs = indices_for(postfix, model)    \n",
    "    avg_grad = (grads[idxs[0]] + grads[idxs[1]] + grads[idxs[2]]) / 3\n",
    "    for i in idxs:\n",
    "        grads[i] = avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e7ea1-3a7a-42ec-af05-05b18eedf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same loop but with explicit gradient tape and tied weights / gradients\n",
    "\n",
    "# create new model\n",
    "train_model = create_configured_keras_model(TRAIN_SEQ_LEN, all_outputs=False)\n",
    "\n",
    "# copy c1 weights into c2 and c3\n",
    "c1a_w = train_model.get_layer('c1a').get_weights()\n",
    "c1b_w = train_model.get_layer('c1b').get_weights()\n",
    "train_model.get_layer('c2a').set_weights(c1a_w)\n",
    "train_model.get_layer('c2b').set_weights(c1b_w)\n",
    "train_model.get_layer('c3a').set_weights(c1a_w)\n",
    "train_model.get_layer('c3b').set_weights(c1b_w)\n",
    "        \n",
    "optimiser = Adam(1e-4) #, loss=masked_mse)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"epoch\", epoch)\n",
    "    \n",
    "    for x_b, y_b in train_ds:        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = train_model(x_b, training=True)\n",
    "            loss = masked_mse(y_b, y_pred)            \n",
    "        grads = tape.gradient(loss, train_model.trainable_weights)\n",
    "\n",
    "        # calculate averages for cNa and cNb, excluding first c0a, c0b\n",
    "        average_gradients_for('a/kernel', train_model, grads)\n",
    "        average_gradients_for('a/bias', train_model, grads)\n",
    "        average_gradients_for('b/kernel', train_model, grads)\n",
    "        average_gradients_for('b/bias', train_model, grads)\n",
    "        \n",
    "        optimiser.apply_gradients(zip(grads, train_model.trainable_weights))\n",
    "            \n",
    "    print(\"end epoch, final loss\", loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7493c36-9527-42c1-b140-65fd6ff991cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.compile(Adam(1e-4), loss=masked_mse)\n",
    "train_model.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39227ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.load_weights(\"weights/003-0.21019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6d1fc-0c81-4c6c-a634-8a338beea902",
   "metadata": {},
   "source": [
    "# check against test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3519e-d3e3-4278-a13b-9d8ea56b82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_coords(wave):\n",
    "    return {'sine': '(0, 0)', 'ramp': '(0, 1)',\n",
    "            'square': '(1, 0)', 'zigzag': '(1, 1)' }[wave]\n",
    "\n",
    "for wave in ['sine', 'ramp', 'square', 'zigzag']:\n",
    "        \n",
    "    test_records = []\n",
    "    for i in range(1000, 1500):\n",
    "        test_seq = tri_to[wave]['test']['x'][i:i+TRAIN_SEQ_LEN]\n",
    "        test_seq = np.expand_dims(test_seq, 0)  # single element batch\n",
    "        \n",
    "        y_true = tri_to[wave]['test']['y'][i+TRAIN_SEQ_LEN]\n",
    "                \n",
    "        y_pred = train_model(test_seq).numpy()\n",
    "        y_pred = y_pred[0,-1,:]  # train model gives all steps, we just want last\n",
    "        \n",
    "        for out_c in range(1):\n",
    "            test_records.append((i, out_c, 'x', test_seq[0,-1,2])) \n",
    "            test_records.append((i, out_c, 'y_true', y_true[out_c]))\n",
    "            test_records.append((i, out_c, 'y_pred', y_pred[out_c]))\n",
    "        \n",
    "    test_df = pd.DataFrame(test_records, columns=['n', 'c', 'name', 'value'])\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    ax = sns.lineplot(data=test_df[test_df['c']==0], x='n', y='value', hue='name')\n",
    "    ax.set(title=f\"test performance on {wave} {wave_coords(wave)}\")\n",
    "    ax.set_ylim(-1, 1)\n",
    "    plt.savefig(f\"/tmp/test_{wave}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f023dff-7403-471e-8e0b-e6299e8bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in ['sine']:   # though we'll overridde x2 and x3\n",
    "        \n",
    "    test_records = []\n",
    "    for i in range(1000, 1500):\n",
    "        test_seq = tri_to[wave]['test']['x'][i:i+TRAIN_SEQ_LEN]\n",
    "        test_seq = np.expand_dims(test_seq, 0)  # single element batch\n",
    "\n",
    "        test_seq[:,:,0] = 0.2  # override x2\n",
    "        test_seq[:,:,1] = 0.0  # override x3\n",
    "        \n",
    "        y_true = tri_to[wave]['test']['y'][i+TRAIN_SEQ_LEN]\n",
    "                \n",
    "        y_pred = train_model(test_seq).numpy()\n",
    "        y_pred = y_pred[0,-1,:]  # train model gives all steps, we just want last\n",
    "        \n",
    "        for out_c in range(1):\n",
    "            test_records.append((i, out_c, 'x', test_seq[0,-1,2])) \n",
    "            test_records.append((i, out_c, 'y_true', y_true[out_c]))\n",
    "            test_records.append((i, out_c, 'y_pred', y_pred[out_c]))\n",
    "        \n",
    "    test_df = pd.DataFrame(test_records, columns=['n', 'c', 'name', 'value'])\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    ax = sns.lineplot(data=test_df[test_df['c']==0], x='n', y='value', hue='name')\n",
    "    ax.set(title=f\"test performance on (0.2, 0) \")\n",
    "    ax.set_ylim(-1, 1)\n",
    "    plt.savefig(f\"/tmp/test_inbetween.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8feee-8d21-40f7-a65c-f6b0e49fce35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe473b9-fcc3-44ad-b4e8-12027f375bbe",
   "metadata": {},
   "source": [
    "## generate code\n",
    "\n",
    "hacktastically generate some blocks of c++ code for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36952205-cba1-4474-ba77-8f8d0500a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 => 3 conv blocks, 10 => 4 conv blocks\n",
    "assert len(train_model.layers) in [8, 10], len(train_model.layers)\n",
    "\n",
    "# layer[0] is input\n",
    "\n",
    "blocks = [\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[1].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[1].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[2].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[2].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[3].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[3].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[4].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[4].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = train_model.layers[5].weights[0].numpy(),\n",
    "        c1_bias = train_model.layers[5].weights[1].numpy(),\n",
    "        c2_kernel = train_model.layers[6].weights[0].numpy(),\n",
    "        c2_bias = train_model.layers[6].weights[1].numpy(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "if len(train_model.layers) == 10:\n",
    "    blocks.append(\n",
    "        Block(\n",
    "            c1_kernel = train_model.layers[7].weights[0].numpy(),\n",
    "            c1_bias = train_model.layers[7].weights[1].numpy(),\n",
    "            c2_kernel = train_model.layers[8].weights[0].numpy(),\n",
    "            c2_bias = train_model.layers[8].weights[1].numpy(),\n",
    "        ))\n",
    "\n",
    "regression = Regression(\n",
    "    weights=train_model.layers[-1].weights[0].numpy()[0],\n",
    "    biases=train_model.layers[-1].weights[1].numpy()   \n",
    ")\n",
    "\n",
    "# create CachedBlockModel since it creates correct layer\n",
    "# caches\n",
    "cached_block_model = CachedBlockModel(\n",
    "    blocks=blocks,\n",
    "    input_feature_depth=IN_D,\n",
    "    regression=regression\n",
    ")      \n",
    "\n",
    "with open(\"/tmp/model_defn.h\", 'w') as f:\n",
    "    cached_block_model.write_model_defn_h(f) #sys.stdout)\n",
    "\n",
    "print(\"LGTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc641867-bcc1-4861-8afb-5622ec67595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_strided_model(\n",
    "        4**4, in_d=3, filter_sizes=[4, 8, 8, 12],\n",
    "        kernel_size=4, out_d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423572c-aa9d-44c2-a9a6-9211d7f2cfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
