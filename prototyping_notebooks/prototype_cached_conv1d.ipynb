{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a73d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cmsisdsp as dsp\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/mat/dev/cached_dilated_causal_convolutions/') \n",
    "\n",
    "from cmsisdsp_py_version.block import Block\n",
    "from cmsisdsp_py_version.keras_model import create_dilated_model\n",
    "from cmsisdsp_py_version.cached_block_model import CachedBlockModel, Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "records_long = []\n",
    "records_wide = []\n",
    "n = 0\n",
    "for line in open('../serial_dump_from_daisy.txt', 'r'):\n",
    "    try:\n",
    "        if line.startswith('b'): \n",
    "            cv = float(line.split(\" \")[2])\n",
    "        else:\n",
    "            in_v, out_v = map(float, line.split(\" \"))\n",
    "            records_long.append((n, 'cv', cv))\n",
    "            records_long.append((n, 'in_v', in_v))\n",
    "            records_long.append((n, 'out_v', out_v))\n",
    "            records_wide.append((n, cv, in_v, out_v))\n",
    "            n += 1\n",
    "    except Exception as e:\n",
    "        print(f\"? [{line.strip()}] ({str(e)})\")\n",
    "df_long = pd.DataFrame(records_long, columns=['n', 'name', 'val'])\n",
    "df_wide = pd.DataFrame(records_wide, columns=['n', 'cv', 'in_v', 'out_v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9327c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.lineplot(df_wide, x='n', y='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.lineplot(df_long[11000:13000], x='n', y='val', hue='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf17433",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = np.array(df_wide['cv'])\n",
    "in_vs = np.array(df_wide['in_v'])\n",
    "x = np.stack([cvs, in_vs]).transpose()\n",
    "\n",
    "y_true = np.expand_dims(np.array(df_wide['out_v']), -1)\n",
    "\n",
    "split = int(len(x) * 0.8)\n",
    "\n",
    "print(split, cvs.shape, in_vs.shape, x.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the purpose of playing with 2d output just add a variant on y_true\n",
    "y_true2 = -y_true/2\n",
    "y_true.shape, y_true2.shape\n",
    "y_true = np.stack([y_true, y_true2], axis=-1).squeeze()\n",
    "y_true.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffbf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = x[:split], y_true[:split]\n",
    "test_x, test_y = x[split:], y_true[split:]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1763ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_D = 2             # input depth\n",
    "K = 4                # kernel size and implied dilation rate\n",
    "FILTER_SIZES = [4, 8, 8]  # filters for Nth layer Kx1 and 1x1 convs\n",
    "OUT_D = 2\n",
    "\n",
    "TEST_SEQ_LEN = K**len(FILTER_SIZES)   # a**b for kernel size a, and b stacked layers\n",
    "TRAIN_SEQ_LEN = int(TEST_SEQ_LEN * 1.5)\n",
    "\n",
    "print(\"TRAIN_SEQ_LEN\", TRAIN_SEQ_LEN)\n",
    "print(\"TEST_SEQ_LEN\", TEST_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configured_keras_model(seq_len, all_outputs):\n",
    "    return create_dilated_model(\n",
    "        seq_len, in_d=IN_D, filter_sizes=FILTER_SIZES,\n",
    "        kernel_size=K, out_d=OUT_D,\n",
    "        all_outputs=all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = create_configured_keras_model(TRAIN_SEQ_LEN, all_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "assert len(train_x) > TRAIN_SEQ_LEN\n",
    "\n",
    "def gen():    \n",
    "    for i in range(len(train_x)-TRAIN_SEQ_LEN-1):\n",
    "        x = train_x[i:i+TRAIN_SEQ_LEN]\n",
    "        y = train_y[i+1:i+1+TRAIN_SEQ_LEN]\n",
    "        yield x, y\n",
    "                 \n",
    "ds = tf.data.Dataset.from_generator(gen, \n",
    "    output_signature=(tf.TensorSpec(shape=(TRAIN_SEQ_LEN, IN_D), dtype=tf.float32),\n",
    "                      tf.TensorSpec(shape=(TRAIN_SEQ_LEN, OUT_D), dtype=tf.float32)))\n",
    "ds = ds.cache().shuffle(1000).batch(32)\n",
    "train_model.compile(Adam(1e-4), loss='mse')\n",
    "train_model.fit(ds, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd379648",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = create_configured_keras_model(TEST_SEQ_LEN, all_outputs=True)\n",
    "test_model.set_weights(train_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756471c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = np.expand_dims(test_x[10:10+TEST_SEQ_LEN], 0)\n",
    "\n",
    "assert test_seq.shape == (1, TEST_SEQ_LEN, IN_D)\n",
    "\n",
    "test_seq[0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = test_model(test_seq)\n",
    "model_out = [v.numpy() for v in model_out]\n",
    "model_out = [v[0] for v in model_out]            # drop batch, which is always 1\n",
    "all_steps_y_pred = model_out[-1]\n",
    "all_steps_y_pred[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1a_out, c1b_out, c2a_out, c2b_out, c3a_out, c3b_out, \n",
    "y_pred_out = model_out[-1]\n",
    "y_pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_step_y_pred = all_steps_y_pred[-1]\n",
    "final_step_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_y[10+TEST_SEQ_LEN]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707f21e",
   "metadata": {},
   "source": [
    "# caching\n",
    "\n",
    "introduce a rolling cache so layer 0 and 1 need only be called once per `apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a736ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import cmsisdsp as dsp\n",
    "from cmsisdsp_py_version.rolling_cache import RollingCache\n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, weights, biases):\n",
    "        print(\">Classifier weights\", weights.shape, \"biases\", biases.shape)\n",
    "        assert len(weights.shape) == 2\n",
    "        self.input_dim = weights.shape[0]\n",
    "        self.output_dim = weights.shape[1]\n",
    "        assert biases.shape == (self.output_dim,)\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def apply(self, x):\n",
    "        assert x.shape == (self.input_dim,)\n",
    "        x_mi = x.reshape((1, self.input_dim))\n",
    "        weights_mi = self.weights\n",
    "        _status, result = dsp.arm_mat_mult_f32(x_mi, weights_mi)\n",
    "        return dsp.arm_add_f32(result, self.biases)\n",
    "    \n",
    "class FixedSizeCachedBlockModel(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 blocks: List[Block],\n",
    "                 input_feature_depth: int,\n",
    "                 classifier: Classifier):\n",
    "\n",
    "        # FixedSizeCachedBlockModel is hacky version that manually runs things\n",
    "        # as a sanity chech.\n",
    "        # block0 and block1 will have caches, but not block2\n",
    "        assert len(blocks) == 3\n",
    "\n",
    "        self.blocks = blocks\n",
    "        self.classifier = classifier\n",
    "\n",
    "        self.kernel_size = blocks[0].kernel_size\n",
    "        self.input_feature_depth = input_feature_depth\n",
    "\n",
    "        # buffer for layer0 input\n",
    "        self.input = np.zeros((self.kernel_size,\n",
    "                               self.input_feature_depth), dtype=np.float32)\n",
    "\n",
    "        self.layer_caches = [\n",
    "          RollingCache(\n",
    "            depth=self.blocks[0].output_feature_depth(),\n",
    "            dilation=self.kernel_size,\n",
    "            kernel_size=self.kernel_size),\n",
    "          RollingCache(\n",
    "            depth=self.blocks[1].output_feature_depth(),\n",
    "            dilation=self.kernel_size**2,\n",
    "            kernel_size=self.kernel_size)\n",
    "        ]\n",
    "\n",
    "    def apply(self, x):\n",
    "        assert x.shape == (self.input_feature_depth,), x.shape\n",
    "\n",
    "        # shift input values left, and add new entry to idx -1\n",
    "        for i in range(self.kernel_size-1):\n",
    "            self.input[i] = self.input[i+1]\n",
    "        self.input[self.kernel_size-1] = x\n",
    "\n",
    "        feature_map = self.input\n",
    "\n",
    "        block_output = self.blocks[0].apply(feature_map)\n",
    "        self.layer_caches[0].add(block_output)\n",
    "        feature_map = self.layer_caches[0].cached_dilated_values()\n",
    "\n",
    "        block_output = self.blocks[1].apply(feature_map)\n",
    "        self.layer_caches[1].add(block_output)\n",
    "        feature_map = self.layer_caches[1].cached_dilated_values()\n",
    "\n",
    "        feature_map = self.blocks[-1].apply(feature_map)\n",
    "\n",
    "        # run y_pred\n",
    "        y_pred = self.classifier.apply(feature_map)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(object):\n",
    "\n",
    "#     def __init__(self, weights, biases):\n",
    "#         assert len(weights.shape) == 2\n",
    "#         self.input_dim = weights.shape[0]\n",
    "#         self.output_dim = weights.shape[1]\n",
    "#         assert biases.shape == (weights.shape[1],)\n",
    "#         self.weights = weights\n",
    "#         self.biases = biases\n",
    "\n",
    "#     def apply(self, x):\n",
    "#         assert x.shape == (self.input_dim,)\n",
    "#         x_mi = x.reshape((1, self.input_dim))\n",
    "#         weights_mi = self.weights\n",
    "#         _status, result = dsp.arm_mat_mult_f32(x_mi, weights_mi)\n",
    "#         return dsp.arm_add_f32(result, self.biases)\n",
    "                \n",
    "assert len(test_model.layers) == 8\n",
    "\n",
    "blocks = [\n",
    "    Block(\n",
    "        c1_kernel = test_model.layers[1].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[1].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[2].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[2].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = test_model.layers[3].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[3].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[4].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[4].weights[1].numpy(),\n",
    "    ),\n",
    "    Block(\n",
    "        c1_kernel = test_model.layers[5].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[5].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[6].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[6].weights[1].numpy(),\n",
    "    )\n",
    "]\n",
    "\n",
    "classifier = Classifier(\n",
    "    weights=test_model.layers[7].weights[0].numpy()[0],\n",
    "    biases=test_model.layers[7].weights[1].numpy()   \n",
    ")\n",
    "\n",
    "cached_block_model = FixedSizeCachedBlockModel(\n",
    "    blocks=blocks,\n",
    "    input_feature_depth=IN_D,\n",
    "    classifier=classifier\n",
    ")      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11479e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.apply(np.array([0.3, 0.1, -0.4, -0.1, 0.5, 0.9, -0.2, 0.6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test model with a sequence > receptive field of the model\n",
    "\n",
    "LONGER_TEST_SEQ_LEN = int(TEST_SEQ_LEN * 1.5)\n",
    "assert LONGER_TEST_SEQ_LEN > TEST_SEQ_LEN\n",
    "\n",
    "longer_test_model = create_configured_keras_model(LONGER_TEST_SEQ_LEN, all_outputs=True)\n",
    "longer_test_model.set_weights(test_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a longer test sequence through the keras model\n",
    "# note: we expect a warmup of 3**3=27 steps for this 3 layer\n",
    "# network as it processed the left padded zeros\n",
    "\n",
    "longer_test_seq = np.expand_dims(test_x[:LONGER_TEST_SEQ_LEN], 0)\n",
    "assert longer_test_seq.shape == (1, LONGER_TEST_SEQ_LEN, 2)\n",
    "\n",
    "model_out = longer_test_model(longer_test_seq)\n",
    "model_out = [v.numpy() for v in model_out]\n",
    "model_out = [v[0] for v in model_out]            # drop batch, which is always 1\n",
    "#c1a_output, c1b_output, c2a_output, c2b_output, c3a_output, c3b_output, \n",
    "y_pred_keras = model_out[-1]\n",
    "y_pred_keras[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964859b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longer_test_seq.shape)\n",
    "\n",
    "y_preds = []\n",
    "for i in range(LONGER_TEST_SEQ_LEN):    \n",
    "    next_step_y_pred = cached_block_model.apply(longer_test_seq[0, i])\n",
    "    y_preds.append(next_step_y_pred)\n",
    "\n",
    "#print(\"final\", final_block_out, final_block_out)\n",
    "\n",
    "y_preds = np.stack(y_preds)\n",
    "y_preds[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isclose(y_preds[-10:], y_pred_keras[-10:], atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1_kernel = test_model.layers[3].weights[0].numpy()\n",
    "# c1_bias = test_model.layers[3].weights[1].numpy()\n",
    "# c2_kernel = test_model.layers[4].weights[0].numpy()\n",
    "# c2_bias = test_model.layers[4].weights[1].numpy()\n",
    "\n",
    "# print(c1_kernel.shape, c1_bias.shape, c2_kernel.shape, c2_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9fe25",
   "metadata": {},
   "source": [
    "## exporting to c statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ca(a):\n",
    "    shapes_as_product = \"*\".join(map(str, a.shape))\n",
    "    return \"[\" + shapes_as_product + \"] = {\" + \", \".join(map(str, a.flatten().tolist())) + \"};\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da058261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_input_buffer_dec():\n",
    "    print(\"LeftShiftBuffer left_shift_input_buffer(\")\n",
    "    print(f\"    {K},   // kernel size\")\n",
    "    print(f\"    {IN_D});  // feature depth\")\n",
    "\n",
    "def print_block_declarations(n, block):\n",
    "    print(f\"float b{n}_c1_kernel{ca(block.c1_kernel)}\")\n",
    "    print(f\"float b{n}_c1_bias{ca(block.c1_bias)}\")\n",
    "    print(f\"float b{n}_c2_kernel{ca(block.c2_kernel)}\")\n",
    "    print(f\"float b{n}_c2_bias{ca(block.c2_bias)}\")\n",
    "    print(f\"Block block{n}({block.kernel_size}, // kernel_size\")\n",
    "    print(f\"             {block.in_d}, {block.c2_out_d}, // in_d, out_d\")\n",
    "    print(f\"             b{n}_c1_kernel, b{n}_c1_bias, b{n}_c2_kernel, b{n}_c2_bias);\")\n",
    "    print()\n",
    "\n",
    "def print_layer_cache_declarations(n, lc):\n",
    "    print(f\"float layer{n}_cache_buffer[{lc.depth}*{lc.dilation}*{lc.kernel_size}];\")\n",
    "    print(f\"RollingCache layer_{n}_cache(\")\n",
    "    print(f\"  {lc.depth}, // depth\")\n",
    "    print(f\"  {lc.dilation}, // dilation\")\n",
    "    print(f\"  {lc.kernel_size}, // kernel size\")\n",
    "    print(f\"  layer_{n}_cache_buffer\")\n",
    "    print(f\");\")\n",
    "    print()\n",
    "    \n",
    "def print_classifier_declarations():\n",
    "    print(f\"float classifier_weights{ca(classifier.weights)}\")\n",
    "    print(f\"float classifier_biases{ca(classifier.biases)}\")    \n",
    "    print(f\"Classifier classifier(\")\n",
    "    print(f\"  {classifier.input_dim}, // input_dim\")\n",
    "    print(f\"  {classifier.output_dim}, // output_dim\")\n",
    "    print(f\"  classifier_weights,\")\n",
    "    print(f\"  classifier_biases\")\n",
    "    print(f\");\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a011289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_input_buffer_dec()\n",
    "# print_block_declarations(0, blocks[0])\n",
    "# print_block_declarations(1, blocks[1])\n",
    "# print_block_declarations(2, blocks[2])\n",
    "# print_layer_cache_declarations(0, cached_block_model.layer_caches[0])\n",
    "# print_layer_cache_declarations(1, cached_block_model.layer_caches[1])\n",
    "# print_classifier_declarations()  # includes output buffer dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  block of test values\n",
    "N = 64\n",
    "\n",
    "print(f\"float test_x{ca(test_x[:N])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{0.3,0.1,0.4,0.1,0.5,0.9,0.2,0.6};\n",
    "\n",
    "block0 = Block(\n",
    "        c1_kernel = test_model.layers[1].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[1].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[2].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[2].weights[1].numpy(),\n",
    "    )\n",
    "block1 = Block(\n",
    "        c1_kernel = test_model.layers[3].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[3].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[4].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[4].weights[1].numpy(),\n",
    "    )\n",
    "block2 = Block(\n",
    "        c1_kernel = test_model.layers[5].weights[0].numpy(),\n",
    "        c1_bias = test_model.layers[5].weights[1].numpy(),\n",
    "        c2_kernel = test_model.layers[6].weights[0].numpy(),\n",
    "        c2_bias = test_model.layers[6].weights[1].numpy(),\n",
    "    )\n",
    "\n",
    "layer0_cache = RollingCache(\n",
    "        depth=block0.output_feature_depth(),\n",
    "            dilation=4, \n",
    "            kernel_size=4)\n",
    "layer1_cache = RollingCache(\n",
    "        depth=block1.output_feature_depth(),\n",
    "            dilation=4*4, \n",
    "            kernel_size=4)\n",
    "\n",
    "foo = True\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    if foo:\n",
    "        inp = np.array([[0.3,0.1],[0.4,0.1],[0.5,0.9],[0.2,0.6]])        \n",
    "    else:\n",
    "        inp = np.array([[0.1,0.5],[0.9,0.2],[0.6,0.3],[0.1,0.4]])\n",
    "    foo = not foo;\n",
    "\n",
    "    b0_result = block0.apply(inp)\n",
    "    layer0_cache.add(b0_result)\n",
    "    result0 = layer0_cache.cached_dilated_values()\n",
    "    \n",
    "    b1_result = block1.apply(result0)\n",
    "    layer1_cache.add(b1_result)\n",
    "    result1 = layer1_cache.cached_dilated_values()\n",
    "\n",
    "    b2_result = block2.apply(result1)\n",
    "    \n",
    "    final_result = classifier.apply(b2_result)\n",
    "    print(\"final_result\", final_result.shape, final_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f11aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
